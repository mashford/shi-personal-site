# Crawl Budget

**Crawl Budget（爬取预算 / 抓取预算 / 檢索預算）** 是搜索引擎（主要是 Google）在**一定时间段内**愿意且能够为你的网站分配的爬取资源总量。简单说，就是 Googlebot（谷歌爬虫）每天/每周愿意花多少时间和请求量来访问、抓取和处理你网站上的页面。

Google 官方的定义（来自 developers.google.com）：

> Crawl budget 是 Googlebot **能爬取（crawl capacity）** 且 **想爬取（crawl demand）** 的 URL 集合。

它由两大核心因素决定：

### 1. Crawl Capacity（爬取容量 / 爬取速率上限）

这是 Googlebot 技术上能承受的爬取量，主要受以下影响：

- 你的服务器响应速度（页面加载快 → 爬虫能多爬）
- 网站是否健康（无大量 5xx 错误、超时、重定向链过长）
- 服务器是否能承受高并发请求（不让爬虫把你的站搞崩）
  Google 会动态调整：如果你的服务器很慢，它就会少爬；反之则多爬。

### 2. Crawl Demand（爬取需求）

这是 Google **想不想** 多爬你的站，主要看：

- 网站整体权威性（域名权重高 → 需求大）
- 内容更新频率（经常有新鲜内容 → 需求大）
- 页面重要性（热门页面、内部链接指向多 → 优先爬）
- 用户信号（点击率、停留时间好 → 间接提升需求）
- 历史爬取表现（之前爬取的内容质量高 → 愿意再来）

### 举个直观的例子

- 小型博客：只有几百页，Google 几乎每次访问都爬完 → crawl budget 基本不成问题。
- 大型电商/新闻站：有 100 万+ 页面，每天 Google 只爬 10 万页 → crawl budget 就很关键。
  - 如果 Google 把预算浪费在低质量页面（参数 URL、重复内容、过期活动页）上，新产品页或重要文章可能好几天甚至几周都爬不到，导致无法及时索引 → 排名慢、流量损失。

### 为什么多语言站点特别需要关注 crawl budget？

正如我们之前聊的，多语言站点 URL 数量往往成倍增加（/en/、/fr/、/de/ 等），很容易把 crawl budget “撑爆”。如果不优化，Google 可能：

- 漏掉某些语言版本的新内容
- 优先爬取低价值页面（比如自动生成的过滤页）
- 导致 hreflang 信号混乱、索引不全

### 什么时候你才真的需要操心 crawl budget？

Google 官方说：大多数小于几千页的站点不用太担心。但如果你遇到以下情况，就该重视：

- Google Search Console 显示大量“已发现 - 未索引”（Discovered - currently not indexed）
- 新页面上线后很久才被索引
- 站点页面数 > 10,000–50,000 且还在增长
- 多语言/多地区版本、电商分类/参数 URL 很多
- 爬取统计报告（Search Console → 设置 → 爬取 → 爬取统计资料）显示每天爬取量远低于页面总数

一句话总结：**Crawl budget 不是让你“多给 Google 预算”，而是让你把有限的预算花在刀刃上**——确保 Google 最先、最多爬取和索引你真正想排名的优质页面。
